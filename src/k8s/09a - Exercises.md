## Exercise 1: Managing Multiple Clusters and Users in `kubeconfig`

### 1.1. Set Up Two “Dummy” Clusters in Your Kubeconfig

Even if you only have one actual cluster, you can create two “cluster” entries in your `~/.kube/config` and simulate switching between them.

1. **Backup your existing kubeconfig** (just in case):

   ```bash
   cp ~/.kube/config ~/.kube/config.backup
   ```

2. **Create two cluster entries**. Run these commands (replace `CLUSTER_CA` with the base64‐encoded CA of your real cluster, or point to the same CA file twice):

   ```bash
   # Cluster “dev-cluster” pointing at the same API server
   kubectl config set-cluster dev-cluster \
     --server=https://127.0.0.1:6443 \
     --certificate-authority=$HOME/.kube/ca.crt \
     --embed-certs=true

   # Cluster “prod-cluster” (point it to the same API server for now)
   kubectl config set-cluster prod-cluster \
     --server=https://127.0.0.1:6443 \
     --certificate-authority=$HOME/.kube/ca.crt \
     --embed-certs=true
   ```

3. **Create two user entries** (you can reference the same client‐cert/key twice, or create dummy ones):

   ```bash
   # “alice” user
   kubectl config set-credentials alice \
     --client-certificate=$HOME/.kube/alice.crt \
     --client-key=$HOME/.kube/alice.key \
     --embed-certs=true

   # “ci-bot” user (use a bearer token instead, e.g. “abc123”)
   kubectl config set-credentials ci-bot \
     --token=abc123
   ```

   > Note: If you don’t have real `alice.crt`/`alice.key`, you can copy your existing `client‐certificate‐data` and `client‐key‐data` from `~/.kube/config` into new files and use those. The goal is simply to practice naming and switching.

4. **Create two contexts** that bind cluster↔user pairs:

   ```bash
   kubectl config set-context dev-alice \
     --cluster=dev-cluster \
     --user=alice \
     --namespace=development

   kubectl config set-context prod-ci \
     --cluster=prod-cluster \
     --user=ci-bot \
     --namespace=ci
   ```

5. **List all contexts**, then switch between them:

   ```bash
   kubectl config get-contexts
   # You should see “dev-alice” and “prod-ci” listed alongside any existing contexts.

   # Switch to “dev-alice”:
   kubectl config use-context dev-alice
   kubectl config current-context    # should print “dev-alice”

   # Switch to “prod-ci”:
   kubectl config use-context prod-ci
   kubectl config current-context    # should print “prod-ci”
   ```

6. **Verify that `kubectl` honors the namespace** in each context. For example:

   ```bash
   # If you switch to dev-alice:
   kubectl config use-context dev-alice
   kubectl get pods      # will list pods in the “development” namespace

   # If you switch to prod-ci:
   kubectl config use-context prod-ci
   kubectl get pods      # will list pods in the “ci” namespace
   ```

   If those namespaces do not exist yet, you can create them:

   ```bash
   kubectl create namespace development
   kubectl create namespace ci
   ```

---

## Exercise 2: Creating and Using Service Accounts

### 2.1. Create a Dedicated ServiceAccount and Consume Its Token

1. **Create a namespace** `sadata` for isolation:

   ```bash
   kubectl create namespace sadata
   ```

2. **Create a ServiceAccount** named `demo-sa` in that namespace. Save the following as `sa.yaml`:

   ```yaml
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: demo-sa
     namespace: sadata
   ```

   Apply it:

   ```bash
   kubectl apply -f sa.yaml
   ```

3. **Retrieve the autogenerated token secret** for `demo-sa` (in Kubernetes v1.24+, service account tokens are projected differently; if your cluster auto‐creates a Secret, you’ll see it here; otherwise create a `Secret` manually—see 2.1.5 below):

   ```bash
   kubectl get serviceaccount demo-sa -n sadata -o yaml
   # Look under “secrets:” for something like “demo-sa-token-xxxxx”
   ```

4. **Extract the token** from the corresponding Secret and save it locally:

   ```bash
   # Suppose the Secret is demo-sa-token-abcde
   kubectl get secret demo-sa-token-abcde -n sadata -o go-template='{{.data.token}}' | base64 -d > demo-sa.token
   ```

5. **(If you’re on Kubernetes ≥1.24 and no token Secret appears automatically)**, create a `Secret` of type `kubernetes.io/service-account-token` that references `demo-sa`:

   ```yaml
   apiVersion: v1
   kind: Secret
   metadata:
     name: demo-sa-token
     annotations:
       kubernetes.io/service-account.name: demo-sa
     namespace: sadata
   type: kubernetes.io/service-account-token
   ```

   Save as `sa-secret.yaml`, then:

   ```bash
   kubectl apply -f sa-secret.yaml
   # Wait a few seconds until the token appears:
   kubectl get secret demo-sa-token -n sadata -o yaml
   kubectl get secret demo-sa-token -n sadata -o go-template='{{.data.token}}' | base64 -d > demo-sa.token
   ```

6. **Create a kubeconfig entry** for `demo-sa` so you can authenticate as that service account. First, point to the same cluster (replace `CLUSTER_NAME` and `API_SERVER`/`CA_DATA` with your values):

   ```bash
   # Set the cluster entry (if it doesn’t already exist):
   kubectl config set-cluster my-cluster \
     --server=https://127.0.0.1:6443 \
     --certificate-authority=$HOME/.kube/ca.crt \
     --embed-certs=true \
     --kubeconfig=sa-kubeconfig

   # Set a user entry that uses the token we extracted:
   kubectl config set-credentials demo-sa-user \
     --token="$(cat demo-sa.token)" \
     --kubeconfig=sa-kubeconfig

   # Create a context for the service account in namespace sadata:
   kubectl config set-context sa-sadata \
     --cluster=my-cluster \
     --user=demo-sa-user \
     --namespace=sadata \
     --kubeconfig=sa-kubeconfig

   # Switch to that context:
   kubectl config use-context sa-sadata --kubeconfig=sa-kubeconfig
   ```

7. **Verify** that this service account has only the default “view”‐like rights in `sadata`. For example, try to list Pods (should succeed, even if there are none) and try to delete a Pod in another namespace (should be forbidden):

   ```bash
   # Using the new kubeconfig (sa-kubeconfig), list pods in sadata:
   kubectl --kubeconfig=sa-kubeconfig get pods  # likely “No resources found”
   # Switch to default namespace implicitly:
   kubectl --kubeconfig=sa-kubeconfig get pods --namespace=default
   # If there is a Pod in default/ (e.g. your own “nginx-deploy”), this will probably be “Forbidden.”

   # Try creating a Pod in “default” (should be forbidden):
   kubectl --kubeconfig=sa-kubeconfig run busybox --image=busybox -- sh -c "sleep 300"
   # You should see “Error from server (Forbidden): pods is forbidden: User "system:serviceaccount:sadata:demo-sa" cannot create resource "pods" in API group "" in the namespace "default"”
   ```

8. **Clean up** (if desired):

   ```bash
   kubectl delete namespace sadata
   rm sa-kubeconfig demo-sa.token sa.yaml sa-secret.yaml
   ```

---

## Exercise 3: Creating a “Human” User via X.509 Certificates

You’ll create a key pair, a CSR, sign it with your cluster’s CA, and then configure a kubeconfig context so that `kubectl` can authenticate as “developer.”

> **Prerequisite**: You need access to your cluster’s CA key and cert (`ca.crt` and `ca.key`) or a way to use the Kubernetes CSR API that auto‐signs.

### 3.1. Generate a Private Key and CSR for “developer”

1. **Generate a 2048-bit RSA private key**:

   ```bash
   openssl genrsa -out developer.key 2048
   ```

2. **Create a CSR** with CN=developer and O=dev-team:

   ```bash
   openssl req -new -key developer.key -out developer.csr \
     -subj "/CN=developer/O=dev-team"
   ```

3. **(Option A: Sign the CSR with the cluster’s CA directly)**
   If you have `ca.crt` and `ca.key` on your local workstation (this is typical if you’re the cluster admin):

   ```bash
   openssl x509 -req -in developer.csr \
     -CA /etc/kubernetes/pki/ca.crt \
     -CAkey /etc/kubernetes/pki/ca.key \
     -CAcreateserial \
     -out developer.crt \
     -days 365 \
     -sha256
   ```

    * This produces `developer.crt` signed by your cluster’s CA.

4. **(Option B: Use the Kubernetes CSR API)**
   If you cannot access the CA private key, you can create a Kubernetes CSR object and have the API server sign it:

    1. Base64-encode the CSR and embed it in a Kubernetes CSR resource. Save this as `csr‐dev.yaml`:

       ```yaml
       apiVersion: certificates.k8s.io/v1
       kind: CertificateSigningRequest
       metadata:
         name: developer-csr
       spec:
         request: $(base64 -w0 developer.csr)
         signerName: kubernetes.io/kube-apiserver-client
         usages:
           - client auth
       ```
    2. Apply it:

       ```bash
       kubectl apply -f csr-dev.yaml
       ```
    3. Approve the CSR:

       ```bash
       kubectl certificate approve developer-csr
       ```
    4. Extract the signed certificate:

       ```bash
       kubectl get csr developer-csr -o jsonpath='{.status.certificate}' | base64 -d > developer.crt
       ```

### 3.2. Configure a kubeconfig Entry for “developer”

Assuming you have:

* `developer.crt` (the signed user certificate)
* `developer.key` (the private key)
* `ca.crt` (the cluster’s CA certificate)

Add a new user entry and context to your `~/.kube/config`:

1. **Set the cluster reference** (if you haven’t already done so in Exercise 1):

   ```bash
   kubectl config set-cluster my-cluster \
     --server=https://127.0.0.1:6443 \
     --certificate-authority=$HOME/.kube/ca.crt \
     --embed-certs=true
   ```

2. **Set the credentials for “developer”**:

   ```bash
   kubectl config set-credentials developer \
     --client-certificate=developer.crt \
     --client-key=developer.key \
     --embed-certs=true
   ```

3. **Create a context** that uses “developer” in namespace `development`:

   ```bash
   kubectl config set-context dev-developer \
     --cluster=my-cluster \
     --user=developer \
     --namespace=development
   ```

4. **Switch to that context** and verify your identity:

   ```bash
   kubectl config use-context dev-developer
   kubectl config current-context   # should be dev-developer

   # Attempt to list pods in “development”:
   kubectl get pods
   # If “developer” has no RBAC rights yet, you’ll see “Forbidden: User "developer" cannot list resource "pods"...”

   # Check your authenticated username:
   kubectl auth can-i --list
   # This will show what “developer” can do. Likely no permissions yet.
   ```

### 3.3. Clean Up

```bash
kubectl delete csr developer-csr            # if you used Option B
rm developer.csr developer.crt developer.key
```

---

## Exercise 4: RBAC—Creating Roles, ClusterRoles, and Bindings

Using the `developer` user and the `demo-sa` service account from earlier exercises, create Roles/ClusterRoles and RoleBindings/ClusterRoleBindings that grant specific permissions. Then verify that each principal can only do what you intend.

### 4.1. Create a “read‐only” Role in Namespace `development`

1. **Create a namespace** `development` (if you haven’t already):

   ```bash
   kubectl create namespace development
   ```

2. **Define a Role** that allows only `get`, `list`, and `watch` on Pods and ConfigMaps in `development`. Save as `role-read-dev.yaml`:

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: Role
   metadata:
     name: read-only-pods-configmaps
     namespace: development
   rules:
     - apiGroups: [""]
       resources: ["pods", "configmaps"]
       verbs: ["get", "list", "watch"]
   ```

   Apply it:

   ```bash
   kubectl apply -f role-read-dev.yaml
   ```

3. **Bind this Role to the “developer” user**. Save as `rb-read-dev-developer.yaml`:

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: RoleBinding
   metadata:
     name: bind-dev-read
     namespace: development
   subjects:
     - kind: User
       name: developer
       apiGroup: rbac.authorization.k8s.io
   roleRef:
     kind: Role
     name: read-only-pods-configmaps
     apiGroup: rbac.authorization.k8s.io
   ```

   Apply it:

   ```bash
   kubectl apply -f rb-read-dev-developer.yaml
   ```

4. **Verify as “developer”** (be sure your `kubectl` context is still `dev-developer`):

   ```bash
   # List pods in development → should succeed (or show “No resources found” if none exist):
   kubectl get pods -n development

   # List configmaps in development → should succeed (or show “No resources found”):
   kubectl get configmaps -n development

   # Try to delete a Pod in development → should be forbidden:
   kubectl delete pod some-pod -n development
   # ERROR: pods "some-pod" is forbidden: User "developer" cannot delete resource "pods" in API group "" in the namespace "development"

   # Try to list secrets in development → should be forbidden:
   kubectl get secrets -n development
   ```

5. **Bind the same Role to “demo-sa”** so that the service account inside the same namespace also has read‐only rights. Save as `rb-read-dev-sa.yaml`:

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: RoleBinding
   metadata:
     name: bind-sa-read
     namespace: development
   subjects:
     - kind: ServiceAccount
       name: demo-sa
       namespace: sadata      # note: if you want the service account from namespace sadata to read development, you can—but typically SA and RoleBinding share a namespace. For best practice, create a similarly named SA in “development” or adjust the RoleBinding namespace.
   roleRef:
     kind: Role
     name: read-only-pods-configmaps
     apiGroup: rbac.authorization.k8s.io
   ```

   Apply it:

   ```bash
   kubectl apply -f rb-read-dev-sa.yaml
   ```

6. **Deploy a Pod that uses `demo-sa` in the `development` namespace** (so that it picks up the RoleBinding permissions). Save as `pod-sa-access.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: sa-printer
     namespace: development
   spec:
     serviceAccountName: demo-sa
     containers:
       - name: printer
         image: bitnami/kubectl:latest
         command:
           - sh
           - -c
           - |
             # Attempt to list pods/configmaps in this namespace, then sleep:
             kubectl get pods && kubectl get configmaps
             sleep 3600
   ```

   Apply it:

   ```bash
   kubectl apply -f pod-sa-access.yaml
   ```

7. **Inspect the logs** of that Pod to confirm it could list pods & configmaps:

   ```bash
   kubectl logs sa-printer -n development
   # It should print something like “No resources found” (or a list), not a “Forbidden” error.
   ```

### 4.2. Create a ClusterRole and ClusterRoleBinding for Node Management

1. **Define a ClusterRole** that allows full management of nodes. Save as `cr-node-admin.yaml`:

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRole
   metadata:
     name: node-admin
   rules:
     - apiGroups: [""]
       resources: ["nodes"]
       verbs: ["get", "list", "watch", "patch", "update", "delete"]
   ```

   Apply it:

   ```bash
   kubectl apply -f cr-node-admin.yaml
   ```

2. **Bind that ClusterRole to a group**—for this exercise, you can create a dummy group “ops-team” in your certificate (for “developer” you already set `O=dev-team`; to use “ops-team” you would need a certificate with `O=ops-team`). For simplicity, bind it to the `demo-sa` service account cluster‐wide. Save as `crb-node-admin-sa.yaml`:

   ```yaml
   apiVersion: rbac.authorization.k8s.io/v1
   kind: ClusterRoleBinding
   metadata:
     name: bind-node-admin-sa
   subjects:
     - kind: ServiceAccount
       name: demo-sa
       namespace: sadata
   roleRef:
     kind: ClusterRole
     name: node-admin
     apiGroup: rbac.authorization.k8s.io
   ```

   Apply it:

   ```bash
   kubectl apply -f crb-node-admin-sa.yaml
   ```

3. **Verify** that a Pod running as `demo-sa` can now list and delete nodes. Create a Pod (again in `development`) that runs `kubectl` and attempts to list nodes:
   Save as `pod-sa-nodes.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: sa-node-tester
     namespace: development
   spec:
     serviceAccountName: demo-sa
     containers:
       - name: tester
         image: bitnami/kubectl:latest
         command:
           - sh
           - -c
           - |
             kubectl get nodes
             # Attempt to delete a “dummy” node (replace “kind-worker” with a real node name if you want to test delete)
             # kubectl delete node kind-worker || echo "Delete either succeeded or failed"
             sleep 3600
   ```

   Apply it:

   ```bash
   kubectl apply -f pod-sa-nodes.yaml
   ```

4. **Check logs**:

   ```bash
   kubectl logs sa-node-tester -n development
   # You should see the list of nodes. If you tried a “delete node” command, you might see an error if the node doesn’t exist or if it succeeded.
   ```

5. **Attempt the same as “developer”** (with user certificate). First switch your context back to `dev-developer`, then:

   ```bash
   kubectl get nodes
   # Should be “Forbidden: User "developer" cannot list resource "nodes"”
   ```

### 4.3. Clean Up RBAC Objects

```bash
kubectl delete -f role-read-dev.yaml
kubectl delete -f rb-read-dev-developer.yaml
kubectl delete -f rb-read-dev-sa.yaml
kubectl delete -f pod-sa-access.yaml
kubectl delete -f cr-node-admin.yaml
kubectl delete -f crb-node-admin-sa.yaml
kubectl delete -f pod-sa-nodes.yaml
```

---

## Exercise 5: Admission Controllers in Action

You will see how built-in admission controllers (PodSecurity and ResourceQuota) block or permit resource creation, and how a custom ValidatingAdmissionWebhook could be simulated (using PodSecurity “restricted” policy).

### 5.1. PodSecurity Admission: Enforce “Restricted” in a Namespace

1. **Create a namespace** `secure-ns`:

   ```bash
   kubectl create namespace secure-ns
   ```

2. **Label that namespace** so that PodSecurity enforces `restricted` mode (deny Pods that don’t meet the strictest security contexts). If your cluster is v1.25+ (with PodSecurity built in), run:

   ```bash
   kubectl label namespace secure-ns pod-security.kubernetes.io/enforce=restricted
   kubectl label namespace secure-ns pod-security.kubernetes.io/enforce-version=latest
   ```

3. **Attempt to create a “privileged” Pod** in `secure-ns`. Save this as `privileged-pod.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: privileged-demo
     namespace: secure-ns
   spec:
     containers:
       - name: nginx
         image: nginx:stable
         securityContext:
           privileged: true
   ```

   Apply it:

   ```bash
   kubectl apply -f privileged-pod.yaml
   ```

4. **Observe the error**. It should be something like:

   ```
   Error from server (Forbidden): error when creating "privileged-pod.yaml": admission webhook "podsecurity.k8s.io" denied the request: restricted: privileged mode is not allowed
   ```

5. **Create a minimally compliant Pod** that meets the “restricted” policy. Save as `restricted-pod.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: restricted-demo
     namespace: secure-ns
   spec:
     securityContext:
       runAsNonRoot: true
       runAsUser: 1000
       fsGroup: 2000
     containers:
       - name: busybox
         image: busybox
         command:
           - sh
           - -c
           - echo "Hello from restricted pod"; sleep 3600
         securityContext:
           allowPrivilegeEscalation: false
           readOnlyRootFilesystem: true
           capabilities:
             drop: ["ALL"]
           runAsNonRoot: true
           runAsUser: 1000
   ```

   Apply it:

   ```bash
   kubectl apply -f restricted-pod.yaml
   ```

    * This should succeed, because it abides by “restricted” (non‐root, no capabilities, read‐only root FS, no privileged, and a valid `seccompProfile: RuntimeDefault` is implied if the cluster enforces it).

6. **Clean up**:

   ```bash
   kubectl delete -f privileged-pod.yaml
   kubectl delete -f restricted-pod.yaml
   kubectl delete namespace secure-ns
   ```

### 5.2. Admission Controller: LimitRange and ResourceQuota

You will create a `LimitRange` that enforces default CPU/memory requests, and a `ResourceQuota` that caps total CPU/memory in a namespace. Attempt to create Pods that violate these limits.

1. **Create a namespace** `quota-ns`:

   ```bash
   kubectl create namespace quota-ns
   ```

2. **Define a LimitRange** that forces every container to request at least `100m` CPU and `128Mi` memory, with a maximum of `500m` CPU and `512Mi` memory if unspecified. Save as `limitrange.yaml`:

   ```yaml
   apiVersion: v1
   kind: LimitRange
   metadata:
     name: compute-limits
     namespace: quota-ns
   spec:
     limits:
       - type: Container
         min:
           cpu: "100m"
           memory: "128Mi"
         max:
           cpu: "500m"
           memory: "512Mi"
         defaultRequest:
           cpu: "200m"
           memory: "256Mi"
         default:
           cpu: "300m"
           memory: "384Mi"
         maxLimitRequestRatio:
           cpu: "2"
           memory: "2"
   ```

   Apply it:

   ```bash
   kubectl apply -f limitrange.yaml
   ```

3. **Define a ResourceQuota** that limits total CPU requests to `1` core and total memory requests to `1Gi`, and caps pods to 5. Save as `resourcequota.yaml`:

   ```yaml
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: dev-quota
     namespace: quota-ns
   spec:
     hard:
       pods: "5"
       requests.cpu: "1"
       requests.memory: "1Gi"
   ```

   Apply it:

   ```bash
   kubectl apply -f resourcequota.yaml
   ```

4. **Create a Pod without specifying any resources**. Because of the LimitRange defaults, the Pod will have requests auto‐injected:

   Save as `pod-no-resources.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: lr-default-demo
     namespace: quota-ns
   spec:
     containers:
       - name: busybox
         image: busybox
         command:
           - sh
           - -c
           - echo "Pod with default resources"; sleep 3600
   ```

   Apply it:

   ```bash
   kubectl apply -f pod-no-resources.yaml
   ```

    * **Verify** the Pod was created, then inspect its resource requests:

      ```bash
      kubectl get pod lr-default-demo -n quota-ns -o yaml | grep -A2 resources
      # You should see “requests.cpu: 200m” and “requests.memory: 256Mi” (the defaultRequest), 
      # and “limits.cpu: 300m” “limits.memory: 384Mi” (the default).
      ```

5. **Create a Pod that requests too little CPU** (below the minimum). Save as `pod-too-small.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: lr-too-small
     namespace: quota-ns
   spec:
     containers:
       - name: busybox
         image: busybox
         resources:
           requests:
             cpu: "50m"
             memory: "64Mi"
         command:
           - sh
           - -c
           - echo "Should be rejected"; sleep 3600
   ```

   Attempt to apply:

   ```bash
   kubectl apply -f pod-too-small.yaml
   ```

    * **Observe** the error, e.g.:

      ```
      Error from server (Forbidden): error when creating "pod-too-small.yaml": pods "lr-too-small" is forbidden: minimum cpu usage per Container is 100m
      ```

6. **Create a Pod that requests too much CPU** (exceeds the maximum). Save as `pod-too-large.yaml`:

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: lr-too-large
     namespace: quota-ns
   spec:
     containers:
       - name: busybox
         image: busybox
         resources:
           requests:
             cpu: "1"
             memory: "256Mi"
         command:
           - sh
           - -c
           - echo "Should be rejected"; sleep 3600
   ```

   Attempt to apply:

   ```bash
   kubectl apply -f pod-too-large.yaml
   ```

    * **Observe** the error, e.g.:

      ```
      Error from server (Forbidden): error when creating "pod-too-large.yaml": pods "lr-too-large" is forbidden: maximum cpu limit per Container is 500m
      ```

7. **Create multiple “default” Pods until you exceed the ResourceQuota**. Because each “default” Pod (upon creation) consumes `200m` CPU and `256Mi` memory (due to LimitRange), you can only create 5 such pods before hitting the quota.

   ```bash
   for i in {1..6}; do
     kubectl run lr-demo-$i --image=busybox -n quota-ns -- sh -c "sleep 3600"
     sleep 1
   done
   ```

    * The first four or five should succeed. The sixth will fail with something like:

      ```
      Error from server (Forbidden): error when creating "STDIN": pods "lr-demo-6" is forbidden: exceeded quota: dev-quota, requested: requests.cpu=200m,requests.memory=256Mi, used: requests.cpu=1,requests.memory=1Gi, limited: requests.cpu=1,requests.memory=1Gi
      ```

8. **Clean up**:

   ```bash
   kubectl delete -f limitrange.yaml
   kubectl delete -f resourcequota.yaml
   kubectl delete namespace quota-ns
   ```

---

## Exercise 6 (Bonus): Simulate a Simple ValidatingAdmissionWebhook

> **Note**: This requires you to be able to run a small HTTP service inside the cluster that listens for admission reviews. We’ll use a minimal “echo” webhook written in Go (or you can use a prebuilt image). If that is too heavy, skip to 5.1 and 5.2 above, which illustrate built‐in admission controllers. The purpose here is to see how a custom ValidatingWebhook blocks bad requests.

### 6.1. Deploy a Simple “Deny-all-Deployments” Webhook (Prebuilt)

For demonstration, we’ll deploy a webhook that simply rejects **all** `CREATE` or `UPDATE` requests for `deployments.apps` in any namespace.

1. **Create a namespace** `webhook-demo`:

   ```bash
   kubectl create namespace webhook-demo
   ```

2. **Deploy a minimal “deny‐deployments” server**. We’ll use a public image `admission-example/deny-deployments:latest` (assume this is a tiny HTTP server that always returns `allowed: false` for Deployments). If you don’t have that, you can quickly write a minimal webhook in Go or Python; but for brevity, let’s assume the image exists.

   Save as `deny-deployments-deployment.yaml`:

   ```yaml
   apiVersion: v1
   kind: ServiceAccount
   metadata:
     name: webhook-sa
     namespace: webhook-demo
   ---
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: deny-deployments
     namespace: webhook-demo
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: deny-deployments
     template:
       metadata:
         labels:
           app: deny-deployments
       spec:
         serviceAccountName: webhook-sa
         containers:
           - name: webhook-server
             image: admission-example/deny-deployments:latest
             ports:
               - containerPort: 8443
             # The server expects TLS certificates mounted at /tls
             volumeMounts:
               - name: certs
                 mountPath: /tls
                 readOnly: true
         volumes:
           - name: certs
             secret:
               secretName: deny-deployments-cert
   ---
   apiVersion: v1
   kind: Service
   metadata:
     name: deny-deployments-svc
     namespace: webhook-demo
   spec:
     ports:
       - port: 443
         targetPort: 8443
     selector:
       app: deny-deployments
   ```

3. **Generate a TLS certificate pair** for the webhook server (signed by a root CA you control):

   ```bash
   # 1. Generate server key:
   openssl genrsa -out webhook-server.key 2048

   # 2. Create a CSR for “deny-deployments.webhook-demo.svc”:
   openssl req -new -key webhook-server.key -out webhook-server.csr \
     -subj "/CN=deny-deployments.webhook-demo.svc"

   # 3. Sign the CSR with your local CA:
   openssl x509 -req -in webhook-server.csr \
     -CA $HOME/.kube/ca.crt -CAkey $HOME/.kube/ca.key \
     -CAcreateserial -out webhook-server.crt \
     -days 365 -sha256

   # 4. Create a Kubernetes secret that contains these certs:
   kubectl create secret tls deny-deployments-cert \
     --cert=webhook-server.crt \
     --key=webhook-server.key \
     -n webhook-demo
   ```

4. **Deploy the webhook server**:

   ```bash
   kubectl apply -f deny-deployments-deployment.yaml
   ```

5. **Create a `ValidatingWebhookConfiguration`** that points to your service. Save as `deny-deployments-webhook.yaml`:

   ```yaml
   apiVersion: admissionregistration.k8s.io/v1
   kind: ValidatingWebhookConfiguration
   metadata:
     name: deny-deployments-webhook
   webhooks:
     - name: deny.deployments.example.com
       admissionReviewVersions: ["v1"]
       sideEffects: None
       failurePolicy: Fail
       timeoutSeconds: 5
       clientConfig:
         service:
           name: deny-deployments-svc
           namespace: webhook-demo
           path: "/validate"
         caBundle: $(kubectl get secret deny-deployments-cert -n webhook-demo -o jsonpath='{.data.ca\.crt}')
       rules:
         - operations: ["CREATE", "UPDATE"]
           apiGroups: ["apps"]
           apiVersions: ["v1"]
           resources: ["deployments"]
   ```

   Fill in the `caBundle` by retrieving the CA‐signed certificate (properly base64‐encoded). A quick way:

   ```bash
   CA_BUNDLE=$(kubectl get secret deny-deployments-cert -n webhook-demo -o jsonpath='{.data.ca\.crt}')
   envsubst <<EOF | kubectl apply -f -
   apiVersion: admissionregistration.k8s.io/v1
   kind: ValidatingWebhookConfiguration
   metadata:
     name: deny-deployments-webhook
   webhooks:
     - name: deny.deployments.example.com
       admissionReviewVersions: ["v1"]
       sideEffects: None
       failurePolicy: Fail
       timeoutSeconds: 5
       clientConfig:
         service:
           name: deny-deployments-svc
           namespace: webhook-demo
           path: "/validate"
         caBundle: ${CA_BUNDLE}
       rules:
         - operations: ["CREATE", "UPDATE"]
           apiGroups: ["apps"]
           apiVersions: ["v1"]
           resources: ["deployments"]
   EOF
   ```

6. **Test the webhook** by trying to create a Deployment in any namespace (e.g., `default`):

   ```bash
   cat <<EOF | kubectl apply -f -
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: test-deploy
     namespace: default
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: test
     template:
       metadata:
         labels:
           app: test
       spec:
         containers:
           - name: nginx
             image: nginx
   EOF
   ```

   You should see an error like:

   ```
   Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed calling webhook "deny.deployments.example.com": Post "https://deny-deployments-svc.webhook-demo.svc:443/validate": x509: certificate signed by unknown authority
   ```

   If you correctly provided the `caBundle`, you will see:

   ```
   Error from server (Forbidden): admission webhook "deny.deployments.example.com" denied the request: all Deployments are disallowed by policy
   ```

7. **Clean up**:

   ```bash
   kubectl delete ValidatingWebhookConfiguration deny-deployments-webhook
   kubectl delete -f deny-deployments-deployment.yaml
   kubectl delete namespace webhook-demo
   rm webhook-server.key webhook-server.csr webhook-server.crt
   ```

> **If you cannot deploy a custom webhook**: simply exercise the built‐in `PodSecurity` (Exercise 5.1) and `LimitRange`/`ResourceQuota` (Exercise 5.2) admission controllers, which illustrate how the API server automatically rejects or mutates objects that violate policy.
